#!/bin/bash

# å¢å¼ºç‰ˆæ•°æ®åˆæˆç®¡é“ v5 è¿è¡Œè„šæœ¬
# å®Œæ•´å®ç°æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½
# ç›®æ ‡ï¼šç”Ÿæˆé«˜è´¨é‡é—®ç­”å¯¹ï¼Œæ”¯æŒæ‰¹é‡æ¨ç†å’Œå¢é‡å¤„ç†

echo "ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ•°æ®åˆæˆç®¡é“ v5 - å®Œæ•´ä¼˜åŒ–ç‰ˆ"
echo "ğŸ¯ ç›®æ ‡ï¼šé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡é—®ç­”å¯¹"
echo "ğŸ“… $(date)"
echo "=================================================="

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES='4,5,6,7'  # ä½¿ç”¨4å¼ GPU
export CHUNK_NUM=4
export CHUNK_NUM_MIN=2
export NUM_distract=3
export PROMPT_KEY="deepseek-v2"

# åŸºç¡€é…ç½®
DATA_DIR="data"
FILTERED_DATA_DIR="data_filtered_v5"
OUTPUT_BASE="outputs_v5"
INTERMEDIATE_DIR="${OUTPUT_BASE}/intermediate"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ${OUTPUT_BASE}/{chunks,chunk4,topics,questions,validated_questions,answers,syndatas,stats}
mkdir -p ${INTERMEDIATE_DIR}

echo "ğŸ“ æ•°æ®ç›®å½•é…ç½®:"
echo "   åŸå§‹æ•°æ®: ${DATA_DIR}"
echo "   ç­›é€‰æ•°æ®: ${FILTERED_DATA_DIR}"
echo "   è¾“å‡ºç›®å½•: ${OUTPUT_BASE}"
echo "   ä¸­é—´æ–‡ä»¶: ${INTERMEDIATE_DIR}"
echo ""

# æ€§èƒ½é…ç½®
BATCH_SIZE=32
MAX_WORKERS=4
MODEL_NAME="qwq_32"

echo "âš™ï¸  æ€§èƒ½é…ç½®:"
echo "   æ‰¹é‡å¤§å°: ${BATCH_SIZE}"
echo "   å¹¶å‘æ•°é‡: ${MAX_WORKERS}"
echo "   ä½¿ç”¨æ¨¡å‹: ${MODEL_NAME}"
echo ""

# æ¨¡å¼1ï¼šå®Œæ•´å¢å¼ºç®¡é“ï¼ˆä½¿ç”¨vLLM + æ‰€æœ‰è´¨é‡æ§åˆ¶ï¼‰
echo "ğŸ”¥ æ¨¡å¼1: è¿è¡Œå®Œæ•´å¢å¼ºç®¡é“ï¼ˆvLLM + æ‰¹é‡æ¨ç† + è´¨é‡æ§åˆ¶ï¼‰"
python -m utils.syndata_pipeline_v5 \
  --data_dir "${DATA_DIR}" \
  --filtered_data_dir "${FILTERED_DATA_DIR}" \
  --chunks_path "${OUTPUT_BASE}/chunks/article_chunks_v5_full.json" \
  --chunk4_path "${OUTPUT_BASE}/chunk4/article_chunk4_v5_full.json" \
  --topics_path "${OUTPUT_BASE}/topics/article_topics_v5_full.json" \
  --questions_path "${OUTPUT_BASE}/questions/article_questions_v5_full.json" \
  --validated_questions_path "${OUTPUT_BASE}/validated_questions/article_questions_v5_validated.json" \
  --answers_path "${OUTPUT_BASE}/answers/article_answers_v5_full.json" \
  --syndatas_path "${OUTPUT_BASE}/syndatas/syndatas_v5_full.json" \
  --judge_output_path "${INTERMEDIATE_DIR}/judge_output_v5.jsonl" \
  --question_output_path "${INTERMEDIATE_DIR}/question_output_v5.jsonl" \
  --question_li_output_path "${INTERMEDIATE_DIR}/question_li_output_v5.jsonl" \
  --start_idx 0 \
  --end_idx 100 \
  --max_workers ${MAX_WORKERS} \
  --batch_size ${BATCH_SIZE} \
  --use_vllm \
  --model_name ${MODEL_NAME} \
  --gpu_memory_utilization 0.95 \
  --tensor_parallel_size 4

echo ""
echo "âœ… å®Œæ•´å¢å¼ºç®¡é“æ‰§è¡Œå®Œæˆï¼"
echo ""

# æ¨¡å¼2ï¼šå¿«é€Ÿæ¨¡å¼ï¼ˆè·³è¿‡æ–‡æ¡£ç­›é€‰ï¼Œä½†ä¿ç•™é—®é¢˜éªŒè¯ï¼‰
echo "âš¡ æ¨¡å¼2: è¿è¡Œå¿«é€Ÿæ¨¡å¼ï¼ˆè·³è¿‡æ–‡æ¡£ç­›é€‰ï¼‰"
python -m utils.syndata_pipeline_v5 \
  --data_dir "${DATA_DIR}" \
  --filtered_data_dir "${FILTERED_DATA_DIR}_fast" \
  --chunks_path "${OUTPUT_BASE}/chunks/article_chunks_v5_fast.json" \
  --chunk4_path "${OUTPUT_BASE}/chunk4/article_chunk4_v5_fast.json" \
  --topics_path "${OUTPUT_BASE}/topics/article_topics_v5_fast.json" \
  --questions_path "${OUTPUT_BASE}/questions/article_questions_v5_fast.json" \
  --validated_questions_path "${OUTPUT_BASE}/validated_questions/article_questions_v5_fast_validated.json" \
  --answers_path "${OUTPUT_BASE}/answers/article_answers_v5_fast.json" \
  --syndatas_path "${OUTPUT_BASE}/syndatas/syndatas_v5_fast.json" \
  --question_output_path "${INTERMEDIATE_DIR}/question_output_v5_fast.jsonl" \
  --question_li_output_path "${INTERMEDIATE_DIR}/question_li_output_v5_fast.jsonl" \
  --start_idx 0 \
  --end_idx 100 \
  --skip_document_filter \
  --max_workers ${MAX_WORKERS} \
  --batch_size ${BATCH_SIZE} \
  --use_vllm \
  --model_name ${MODEL_NAME}

echo ""
echo "âœ… å¿«é€Ÿæ¨¡å¼æ‰§è¡Œå®Œæˆï¼"
echo ""

# æ¨¡å¼3ï¼šæ ‡å‡†APIæ¨¡å¼ï¼ˆä¸ä½¿ç”¨vLLMï¼Œé€‚åˆå°è§„æ¨¡æµ‹è¯•ï¼‰
echo "ğŸ”§ æ¨¡å¼3: è¿è¡Œæ ‡å‡†APIæ¨¡å¼ï¼ˆä¸ä½¿ç”¨vLLMï¼‰"
python -m utils.syndata_pipeline_v5 \
  --data_dir "${DATA_DIR}" \
  --filtered_data_dir "${FILTERED_DATA_DIR}_api" \
  --chunks_path "${OUTPUT_BASE}/chunks/article_chunks_v5_api.json" \
  --chunk4_path "${OUTPUT_BASE}/chunk4/article_chunk4_v5_api.json" \
  --topics_path "${OUTPUT_BASE}/topics/article_topics_v5_api.json" \
  --questions_path "${OUTPUT_BASE}/questions/article_questions_v5_api.json" \
  --validated_questions_path "${OUTPUT_BASE}/validated_questions/article_questions_v5_api_validated.json" \
  --answers_path "${OUTPUT_BASE}/answers/article_answers_v5_api.json" \
  --syndatas_path "${OUTPUT_BASE}/syndatas/syndatas_v5_api.json" \
  --start_idx 0 \
  --end_idx 10 \
  --max_workers 2 \
  --batch_size 4

echo ""
echo "âœ… æ ‡å‡†APIæ¨¡å¼æ‰§è¡Œå®Œæˆï¼"
echo ""

# æ¨¡å¼4ï¼šå…¼å®¹æ¨¡å¼ï¼ˆä¸v3å®Œå…¨å…¼å®¹ï¼‰
echo "ğŸ”„ æ¨¡å¼4: è¿è¡Œå…¼å®¹æ¨¡å¼ï¼ˆè·³è¿‡æ‰€æœ‰å¢å¼ºåŠŸèƒ½ï¼‰"
python -m utils.syndata_pipeline_v5 \
  --data_dir "${DATA_DIR}" \
  --filtered_data_dir "${FILTERED_DATA_DIR}_compat" \
  --chunks_path "${OUTPUT_BASE}/chunks/article_chunks_v5_compat.json" \
  --chunk4_path "${OUTPUT_BASE}/chunk4/article_chunk4_v5_compat.json" \
  --topics_path "${OUTPUT_BASE}/topics/article_topics_v5_compat.json" \
  --questions_path "${OUTPUT_BASE}/questions/article_questions_v5_compat.json" \
  --validated_questions_path "${OUTPUT_BASE}/validated_questions/article_questions_v5_compat_validated.json" \
  --answers_path "${OUTPUT_BASE}/answers/article_answers_v5_compat.json" \
  --syndatas_path "${OUTPUT_BASE}/syndatas/syndatas_v5_compat.json" \
  --start_idx 0 \
  --end_idx 10 \
  --skip_document_filter \
  --skip_question_validation \
  --skip_text_preprocessing \
  --max_workers 1 \
  --batch_size 1

echo ""
echo "âœ… å…¼å®¹æ¨¡å¼æ‰§è¡Œå®Œæˆï¼"
echo ""

# ç»Ÿè®¡å’Œå¯¹æ¯”
echo "ğŸ‰ æ‰€æœ‰ç®¡é“æ‰§è¡Œå®Œæˆï¼"
echo ""
echo "ğŸ“Š è¾“å‡ºæ–‡ä»¶å¯¹æ¯”ï¼š"
echo "   å®Œæ•´å¢å¼ºç‰ˆ: ${OUTPUT_BASE}/syndatas/syndatas_v5_full.json"
echo "   å¿«é€Ÿæ¨¡å¼:   ${OUTPUT_BASE}/syndatas/syndatas_v5_fast.json"
echo "   æ ‡å‡†API:    ${OUTPUT_BASE}/syndatas/syndatas_v5_api.json"
echo "   å…¼å®¹æ¨¡å¼:   ${OUTPUT_BASE}/syndatas/syndatas_v5_compat.json"
echo ""
echo "ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡æ–‡ä»¶:"
ls -la ${OUTPUT_BASE}/stats/
echo ""
echo "ğŸ“ ä¸­é—´ç»“æœæ–‡ä»¶:"
ls -la ${INTERMEDIATE_DIR}/
echo ""
echo "ğŸ” å»ºè®®ä½¿ç”¨ cotæ•°æ®è´¨é‡è¯„ä¼°.py å¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè´¨é‡è¯„ä¼°"
echo "ğŸ“… å®Œæˆæ—¶é—´: $(date)"
echo "=================================================="

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
echo ""
echo "ğŸ“‹ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š..."
python -c "
import json
import os

output_base = '${OUTPUT_BASE}'
stats_files = [
    f'{output_base}/syndatas/pipeline_v5_stats.json',
    f'{output_base}/quality_filter_stats.json',
    f'{output_base}/question_generation_stats.json'
]

print('\\n=== ç®¡é“æ‰§è¡Œç»Ÿè®¡ ===')
for stats_file in stats_files:
    if os.path.exists(stats_file):
        with open(stats_file, 'r') as f:
            stats = json.load(f)
        print(f'\\n{os.path.basename(stats_file)}:')
        for key, value in stats.items():
            print(f'  {key}: {value}')
"

echo ""
echo "âœ¨ å…¨éƒ¨å®Œæˆï¼"