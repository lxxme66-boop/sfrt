import os
import sys
import json
import time
import logging
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# å¯¼å…¥åŸºç¡€æ¨¡å—
from utils.common_utils import build_openai_client_chat, load_articles, get_chunkstr
from utils.article_chunks import gen_chunks
from utils.topic_concepts import trans_chunk4, gen_topics, gen_questions_with_topic_v3
from utils.answer_generation import gen_answer_v3
from utils.data_synthesis import syn_data_v2

# å¯¼å…¥å¢å¼ºæ¨¡å—
from utils.document_quality_filter_v5 import filter_high_quality_documents_batch
from utils.enhanced_question_generation_v5 import gen_enhanced_questions_with_validation_batch

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

print(f"Python executable: {sys.executable}")

def get_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å¢å¼ºç‰ˆæ•°æ®åˆæˆç®¡é“ v5 - å®Œæ•´ä¼˜åŒ–å®ç°",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # åŸºç¡€è·¯å¾„å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="åŸå§‹æ–‡æ¡£ç›®å½•è·¯å¾„")
    parser.add_argument("--filtered_data_dir", type=str, default="data_filtered", help="ç­›é€‰åæ–‡æ¡£ä¿å­˜ç›®å½•")
    parser.add_argument("--chunks_path", type=str, required=True, help="æ–‡æ¡£åˆ†å—ä¿å­˜è·¯å¾„")
    parser.add_argument("--chunk4_path", type=str, required=True, help="Chunk4æ•°æ®ä¿å­˜è·¯å¾„")
    parser.add_argument("--topics_path", type=str, required=True, help="ä¸»é¢˜æ•°æ®ä¿å­˜è·¯å¾„")
    parser.add_argument("--questions_path", type=str, required=True, help="åŸå§‹é—®é¢˜ä¿å­˜è·¯å¾„")
    parser.add_argument("--validated_questions_path", type=str, required=True, help="éªŒè¯åé—®é¢˜ä¿å­˜è·¯å¾„")
    parser.add_argument("--answers_path", type=str, required=True, help="ç­”æ¡ˆæ•°æ®ä¿å­˜è·¯å¾„")
    parser.add_argument("--syndatas_path", type=str, required=True, help="æœ€ç»ˆåˆæˆæ•°æ®ä¿å­˜è·¯å¾„")
    
    # å¤„ç†èŒƒå›´å‚æ•°
    parser.add_argument("--start_idx", type=int, default=0, help="å¤„ç†æ–‡æ¡£çš„èµ·å§‹ç´¢å¼•")
    parser.add_argument("--end_idx", type=int, default=None, help="å¤„ç†æ–‡æ¡£çš„ç»“æŸç´¢å¼•")
    
    # åŠŸèƒ½å¼€å…³å‚æ•°
    parser.add_argument("--skip_document_filter", action="store_true", help="è·³è¿‡æ–‡æ¡£è´¨é‡ç­›é€‰æ­¥éª¤")
    parser.add_argument("--skip_question_validation", action="store_true", help="è·³è¿‡é—®é¢˜è´¨é‡éªŒè¯æ­¥éª¤")
    parser.add_argument("--skip_text_preprocessing", action="store_true", help="è·³è¿‡æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤")
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    parser.add_argument("--max_workers", type=int, default=4, help="å¹¶å‘å¤„ç†æ•°é‡")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹é‡æ¨ç†å¤§å°")
    parser.add_argument("--use_vllm", action="store_true", help="ä½¿ç”¨vLLMè¿›è¡Œé«˜æ•ˆæ¨ç†")
    parser.add_argument("--model_name", type=str, default="qwq_32", help="ä½¿ç”¨çš„æ¨¡å‹åç§°")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.95, help="GPUå†…å­˜åˆ©ç”¨ç‡")
    parser.add_argument("--tensor_parallel_size", type=int, default=4, help="å¼ é‡å¹¶è¡Œå¤§å°")
    
    # ä¸­é—´æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå¢é‡å¤„ç†ï¼‰
    parser.add_argument("--judge_output_path", type=str, help="æ–‡æ¡£è¯„ä¼°ç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument("--question_output_path", type=str, help="é—®é¢˜ç”Ÿæˆç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument("--question_li_output_path", type=str, help="å±•å¼€åé—®é¢˜ä¿å­˜è·¯å¾„")
    
    args = parser.parse_args()
    return args

def enhanced_data_synthesis_pipeline_v5(
    data_dir,
    filtered_data_dir,
    chunks_path,
    chunk4_path,
    topics_path,
    questions_path,
    validated_questions_path,
    answers_path,
    syndatas_path,
    start_idx=0,
    end_idx=None,
    skip_document_filter=False,
    skip_question_validation=False,
    skip_text_preprocessing=False,
    max_workers=4,
    batch_size=32,
    use_vllm=False,
    model_config=None,
    intermediate_paths=None
):
    """
    å¢å¼ºç‰ˆæ•°æ®åˆæˆç®¡é“ v5 - å®Œæ•´ä¼˜åŒ–å®ç°
    
    ä¸»è¦æ”¹è¿›ï¼š
    1. æ‰¹é‡æ¨ç†ä¼˜åŒ–ï¼ˆvLLMæ”¯æŒï¼‰
    2. æ–‡æœ¬é¢„å¤„ç†
    3. å¢é‡å¤„ç†
    4. æ›´å¥½çš„é”™è¯¯å¤„ç†
    5. è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ª
    """
    pipeline_start = time.time()
    
    # åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
    if use_vllm:
        logger.info("ğŸš€ åˆå§‹åŒ–vLLMæ¨ç†å¼•æ“...")
        from utils.vllm_client import build_vllm_client
        llm_client = build_vllm_client(model_config)
    else:
        logger.info("ğŸ”§ ä½¿ç”¨æ ‡å‡†OpenAIå®¢æˆ·ç«¯...")
        llm_client = build_openai_client_chat()
    
    # ç¬¬0æ­¥ï¼šæ–‡æ¡£è´¨é‡ç­›é€‰ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
    if not skip_document_filter:
        logger.info("ğŸ” æ­¥éª¤1: å¼€å§‹æ–‡æ¡£è´¨é‡ç­›é€‰ï¼ˆæ‰¹é‡å¤„ç†ï¼‰...")
        filter_start = time.time()
        
        actual_data_dir = filter_high_quality_documents_batch(
            data_dir=data_dir,
            filtered_data_dir=filtered_data_dir,
            llm_client=llm_client,
            batch_size=batch_size,
            max_workers=max_workers,
            skip_preprocessing=skip_text_preprocessing,
            judge_output_path=intermediate_paths.get('judge_output_path') if intermediate_paths else None
        )
        
        filter_time = time.time() - filter_start
        logger.info(f"âœ… æ–‡æ¡£è´¨é‡ç­›é€‰å®Œæˆï¼Œè€—æ—¶: {filter_time/60:.2f}åˆ†é’Ÿ")
    else:
        logger.info("â­ï¸ è·³è¿‡æ–‡æ¡£è´¨é‡ç­›é€‰æ­¥éª¤")
        actual_data_dir = data_dir
    
    # ç¬¬1æ­¥ï¼šæ–‡æ¡£åˆ†å—
    logger.info("ğŸ“„ æ­¥éª¤2: å¼€å§‹æ–‡æ¡£åˆ†å—...")
    chunk_start = time.time()
    
    gen_chunks(actual_data_dir, chunks_path, start_idx, end_idx)
    
    chunk_time = time.time() - chunk_start
    logger.info(f"âœ… æ–‡æ¡£åˆ†å—å®Œæˆï¼Œè€—æ—¶: {chunk_time/60:.2f}åˆ†é’Ÿ")
    
    # ç¬¬2æ­¥ï¼šChunk4è½¬æ¢
    logger.info("ğŸ”„ æ­¥éª¤3: å¼€å§‹Chunk4è½¬æ¢...")
    chunk4_start = time.time()
    
    trans_chunk4(chunks_path, chunk4_path)
    
    chunk4_time = time.time() - chunk4_start
    logger.info(f"âœ… Chunk4è½¬æ¢å®Œæˆï¼Œè€—æ—¶: {chunk4_time/60:.2f}åˆ†é’Ÿ")
    
    # ç¬¬3æ­¥ï¼šä¸»é¢˜ç”Ÿæˆ
    logger.info("ğŸ¯ æ­¥éª¤4: å¼€å§‹ä¸»é¢˜ç”Ÿæˆ...")
    topic_start = time.time()
    
    gen_topics(chunk4_path, topics_path, llm_client)
    
    topic_time = time.time() - topic_start
    logger.info(f"âœ… ä¸»é¢˜ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {topic_time/60:.2f}åˆ†é’Ÿ")
    
    # ç¬¬4æ­¥ï¼šå¢å¼ºç‰ˆé—®é¢˜ç”Ÿæˆå’ŒéªŒè¯ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
    logger.info("â“ æ­¥éª¤5: å¼€å§‹å¢å¼ºç‰ˆé—®é¢˜ç”Ÿæˆå’ŒéªŒè¯ï¼ˆæ‰¹é‡å¤„ç†ï¼‰...")
    question_start = time.time()
    
    if not skip_question_validation:
        gen_enhanced_questions_with_validation_batch(
            topics_path=topics_path,
            questions_path=questions_path,
            validated_questions_path=validated_questions_path,
            llm_client=llm_client,
            chunk4_path=chunk4_path,
            batch_size=batch_size,
            max_workers=max_workers,
            question_output_path=intermediate_paths.get('question_output_path') if intermediate_paths else None,
            question_li_output_path=intermediate_paths.get('question_li_output_path') if intermediate_paths else None
        )
        actual_questions_path = validated_questions_path
    else:
        logger.info("â­ï¸ è·³è¿‡é—®é¢˜è´¨é‡éªŒè¯ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜ç”Ÿæˆ")
        gen_questions_with_topic_v3(topics_path, questions_path, llm_client, chunk4_path)
        actual_questions_path = questions_path
    
    question_time = time.time() - question_start
    logger.info(f"âœ… é—®é¢˜ç”Ÿæˆå’ŒéªŒè¯å®Œæˆï¼Œè€—æ—¶: {question_time/60:.2f}åˆ†é’Ÿ")
    
    # ç¬¬5æ­¥ï¼šç­”æ¡ˆç”Ÿæˆ
    logger.info("ğŸ’¬ æ­¥éª¤6: å¼€å§‹ç­”æ¡ˆç”Ÿæˆ...")
    answer_start = time.time()
    
    gen_answer_v3(actual_questions_path, llm_client, answers_path)
    
    answer_time = time.time() - answer_start
    logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {answer_time/60:.2f}åˆ†é’Ÿ")
    
    # ç¬¬6æ­¥ï¼šæ•°æ®åˆæˆ
    logger.info("ğŸ”— æ­¥éª¤7: å¼€å§‹æ•°æ®åˆæˆ...")
    syndata_start = time.time()
    
    syn_data_v2(answers_path, syndatas_path)
    
    syndata_time = time.time() - syndata_start
    logger.info(f"âœ… æ•°æ®åˆæˆå®Œæˆï¼Œè€—æ—¶: {syndata_time/60:.2f}åˆ†é’Ÿ")
    
    # æ€»ç»“ç»Ÿè®¡
    total_time = time.time() - pipeline_start
    logger.info("ğŸ‰ å¢å¼ºç‰ˆæ•°æ®åˆæˆç®¡é“v5æ‰§è¡Œå®Œæˆï¼")
    logger.info("=" * 50)
    logger.info("â±ï¸  å„æ­¥éª¤è€—æ—¶ç»Ÿè®¡:")
    if not skip_document_filter:
        logger.info(f"   æ–‡æ¡£ç­›é€‰: {filter_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   æ–‡æ¡£åˆ†å—: {chunk_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   Chunk4è½¬æ¢: {chunk4_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   ä¸»é¢˜ç”Ÿæˆ: {topic_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   é—®é¢˜ç”Ÿæˆ: {question_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   ç­”æ¡ˆç”Ÿæˆ: {answer_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   æ•°æ®åˆæˆ: {syndata_time/60:.2f}åˆ†é’Ÿ")
    logger.info(f"   æ€»è€—æ—¶: {total_time/60:.2f}åˆ†é’Ÿ")
    logger.info("=" * 50)
    
    # è¿”å›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    return {
        "filter_time": filter_time if not skip_document_filter else 0,
        "chunk_time": chunk_time,
        "chunk4_time": chunk4_time,
        "topic_time": topic_time,
        "question_time": question_time,
        "answer_time": answer_time,
        "syndata_time": syndata_time,
        "total_time": total_time,
        "use_vllm": use_vllm,
        "batch_size": batch_size
    }

if __name__ == "__main__":
    args = get_args()
    
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ•°æ®åˆæˆç®¡é“ v5")
    logger.info(f"ğŸ“ åŸå§‹æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"ğŸ“ ç­›é€‰æ•°æ®ç›®å½•: {args.filtered_data_dir}")
    logger.info(f"ğŸ“„ æœ€ç»ˆè¾“å‡º: {args.syndatas_path}")
    logger.info(f"ğŸ”§ ä½¿ç”¨vLLM: {args.use_vllm}")
    logger.info(f"ğŸ“¦ æ‰¹é‡å¤§å°: {args.batch_size}")
    
    # åˆ›å»ºå¿…è¦çš„è¾“å‡ºç›®å½•
    for path in [args.chunks_path, args.chunk4_path, args.topics_path,
                 args.questions_path, args.validated_questions_path,
                 args.answers_path, args.syndatas_path]:
        os.makedirs(os.path.dirname(path), exist_ok=True)
    
    # å‡†å¤‡æ¨¡å‹é…ç½®
    model_config = {
        "model_name": args.model_name,
        "gpu_memory_utilization": args.gpu_memory_utilization,
        "tensor_parallel_size": args.tensor_parallel_size
    }
    
    # å‡†å¤‡ä¸­é—´æ–‡ä»¶è·¯å¾„
    intermediate_paths = {
        "judge_output_path": args.judge_output_path,
        "question_output_path": args.question_output_path,
        "question_li_output_path": args.question_li_output_path
    }
    
    try:
        timing_stats = enhanced_data_synthesis_pipeline_v5(
            data_dir=args.data_dir,
            filtered_data_dir=args.filtered_data_dir,
            chunks_path=args.chunks_path,
            chunk4_path=args.chunk4_path,
            topics_path=args.topics_path,
            questions_path=args.questions_path,
            validated_questions_path=args.validated_questions_path,
            answers_path=args.answers_path,
            syndatas_path=args.syndatas_path,
            start_idx=args.start_idx,
            end_idx=args.end_idx,
            skip_document_filter=args.skip_document_filter,
            skip_question_validation=args.skip_question_validation,
            skip_text_preprocessing=args.skip_text_preprocessing,
            max_workers=args.max_workers,
            batch_size=args.batch_size,
            use_vllm=args.use_vllm,
            model_config=model_config,
            intermediate_paths=intermediate_paths
        )
        
        logger.info("âœ¨ ç®¡é“æ‰§è¡ŒæˆåŠŸå®Œæˆï¼")
        
        # ä¿å­˜æ‰§è¡Œç»Ÿè®¡
        stats_path = os.path.join(os.path.dirname(args.syndatas_path), "pipeline_v5_stats.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(timing_stats, f, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ ç®¡é“æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise