{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0832acef",
   "metadata": {},
   "source": [
    "# 离线流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取chunk的embedding\n",
    "from utils.vector_store import get_chunk_with_embedding\n",
    "chunks_path = \"outputs_chunks/article_chunks06.json\"\n",
    "embedding_path = \"outputs_chunks/chunk_embedding08.json\"\n",
    "chunks_with_embedding = get_chunk_with_embedding(\n",
    "    chunks_path, embedding_path=embedding_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from utils.common_utils import build_doubao_embedding\n",
    "import os\n",
    "class DouBaoEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model_name: str = \"doubao-embedding-text-240715\", emb_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_name = model_name\n",
    "        self.emb_model = emb_model\n",
    "    def _get_embedding(self, texts: list[str] | str) -> List[float] | List[List[float]]:\n",
    "        # 这里替换为实际调用豆包平台的 API 获取 embedding 的逻辑\n",
    "        # 例如通过 requests 请求、认证等\n",
    "        single_text = isinstance(texts, str)\n",
    "        if single_text:\n",
    "            texts = [texts]\n",
    "        response = self.emb_model(\n",
    "            model=self.model_name,\n",
    "            input=texts\n",
    "        )\n",
    "        embeddings = [\n",
    "            embedding_data.embedding for embedding_data in response.data\n",
    "        ]\n",
    "        if single_text:\n",
    "            return embeddings[0]\n",
    "        return embeddings  # 返回浮点数列表\n",
    "\n",
    "    async def _aget_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_embedding(text)\n",
    "\n",
    "    def _get_text_embedding(self, text: list[str]) -> List[List[float]]:\n",
    "        return self._get_embedding(text)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_embedding(query)\n",
    "    async def _aget_text_embedding(self, text: list[str]) -> List[List[float]]:\n",
    "        return self._get_text_embedding(text)\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "def get_doubao_embedding(model=\"doubao-embedding-text-240715\"):\n",
    "    emb_model = build_doubao_embedding()\n",
    "    \n",
    "    embedding_model = DouBaoEmbedding(\n",
    "        model=model,\n",
    "        emb_model=emb_model,\n",
    "        api_key=os.environ.get(\"COMPLETION_OPENAI_API_KEY\"),\n",
    "        api_base=os.environ.get(\"COMPLETION_OPENAI_BASE_URL\"),\n",
    "    )\n",
    "    return embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vector_store import get_doubao_embedding\n",
    "\n",
    "doubao_embedding = get_doubao_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 chunks 存储起来\n",
    "# embedding_path = \"outputs_chunks/chunk_embedding08.json\"\n",
    "# allnodes = get_nodes(embedding_path)\n",
    "\n",
    "def storage_embedding_nodes(\n",
    "        embedding_path,\n",
    "        chroma_db=\"llama_index/chroma_db\",\n",
    "        chroma_name=\"sc_collection01\",\n",
    "        storage_dir=\"./vector_index01\",\n",
    "        embedding_model=None\n",
    "    ):\n",
    "    allnodes = get_nodes(embedding_path)\n",
    "    docstore = SimpleDocumentStore()\n",
    "    docstore.add_documents(allnodes)\n",
    "    \n",
    "    db = chromadb.PersistentClient(path=chroma_db)\n",
    "    chroma_collection = db.get_or_create_collection(name=chroma_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store, \n",
    "        docstore=docstore\n",
    "    )\n",
    "    vector_store.add(allnodes)\n",
    "\n",
    "    doubao_embedding = get_doubao_embedding()\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True,\n",
    "        embed_model=doubao_embedding\n",
    "    )\n",
    "    storage_context.persist(persist_dir=storage_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb41d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vector_store import storage_embedding_nodes\n",
    "\n",
    "embedding_path = \"outputs_chunks/chunk_embedding08.json\"\n",
    "storage_embedding_nodes(\n",
    "    embedding_path,\n",
    "    chroma_db=\"llama_index/chroma_db01\",\n",
    "    chroma_name=\"sc_collection\",\n",
    "    storage_dir=\"llama_index/vector_index01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69249931",
   "metadata": {},
   "source": [
    "# 在线检索流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a3af2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': 'hello',\n",
       "  'chunk_id': 2,\n",
       "  'source': 'chunk_2',\n",
       "  'score': 2.1747677326202393},\n",
       " {'chunk': '我叫张三',\n",
       "  'chunk_id': 1,\n",
       "  'source': 'chunk_1',\n",
       "  'score': -1.6213587522506714},\n",
       " {'chunk': 'rainning',\n",
       "  'chunk_id': 3,\n",
       "  'source': 'chunk_3',\n",
       "  'score': -9.40652084350586}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.retrieve_nodes import rerank_chunks\n",
    "question = \"请用中文回答：你好，你叫什么名字？\"\n",
    "docs = [\n",
    "    {\"chunk\": \"我叫张三\", \"chunk_id\": 1, \"source\": \"chunk_1\"}, \n",
    "    {\"chunk\": \"hello\", \"chunk_id\": 2, \"source\": \"chunk_2\"}, \n",
    "    {\"chunk\": \"rainning\", \"chunk_id\": 3, \"source\": \"chunk_3\"}, \n",
    "]\n",
    "sorted_chunks = rerank_chunks(question, docs)\n",
    "sorted_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\raft\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Miniconda3\\envs\\raft\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Retrieving nodes for query: 你是做什么工作的?\n",
      "20 nodes retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\Miniconda3\\envs\\raft\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(response.source_nodes): 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " NodeWithScore(node=TextNode(id_='2244e12b-fdd7-45d5-bb69-459b2c5b429b', embedding=None, metadata={'source': '2005_OLED行业一瞥_王力_llm_correct.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='电话:编辑部 68514016, 58882976广告部 68578428, 68578429, 58882861会展部 68580742, 68511818-25发行部 58882980, 68511818-34, 33\\n电邮:编辑部 article@edw.com.cn广告部 ads@edw.com.cn发行部 faxing@edw.com.cn会展部 seminar@edw.com.cn\\n传真: (010) 68580564 Overseas Agent 海外广告代理:\\n美国地区：AlignPoint Media Inc.\\nTel: 1-925-998-4342, 1-510-828-7899\\nFax: 1-866-235-4856\\nEmail: eepw@alignpoint.com\\n香港地区：Alegra International Ltd.\\nE-mail: eepw@alegra.com.hk\\n日本地区：Chugai Co., Ltd.\\nTel: 81-3-3255-8411 Fax: 81-3-3255-8412 Contact Person: Mizoguchi Hiroyasu', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=-6.721523761749268))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.retrieve_nodes import get_reranked_nodes\n",
    "query = \"你是做什么工作的?\"\n",
    "source_nodes = get_reranked_nodes(query)\n",
    "len(source_nodes), source_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\raft\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Miniconda3\\envs\\raft\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving nodes for query: What is the capital of France?\n",
      "20 nodes retrieved\n"
     ]
    }
   ],
   "source": [
    "from utils.retrieve_nodes import get_retriever\n",
    "\n",
    "retriever = get_retriever(\n",
    "    docstore_path=\"llama_index/docstore.json\",\n",
    "    chroma_db=\"llama_index/chroma_db01\",\n",
    "    chroma_name=\"sc_collection\",\n",
    "    storage_dir=\"llama_index/vector_index01\",\n",
    "    similarity_top_k=10\n",
    ")\n",
    "retrieved_nodes = retriever.retrieve(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b342cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"你是做什么工作的?\"\n",
    "from utils.retrieve_nodes import get_retriever\n",
    "retriever = get_retriever(\n",
    "    docstore_path=\"llama_index/docstore.json\",\n",
    "    chroma_db=\"llama_index/chroma_db01\",\n",
    "    chroma_name=\"sc_collection\",\n",
    "    storage_dir=\"llama_index/vector_index01\",\n",
    "    similarity_top_k=10\n",
    ")\n",
    "# from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "# reranker_model = r\"C:\\Users\\Administrator\\.cache\\modelscope\\hub\\models\\BAAI\\bge-reranker-large\"\n",
    "# reranker = FlagEmbeddingReranker(\n",
    "#     model=reranker_model, top_n=15\n",
    "# )\n",
    "source_nodes = get_reranked_nodes(query, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56606f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reranked_nodes(query, retrieved_nodes, retriever, reranker):\n",
    "    Settings.llm = None\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        llm=None,\n",
    "        response_mode=\"no_text\",\n",
    "        retriever=retriever, \n",
    "        node_postprocessors=[reranker]\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"len(response.source_nodes): {len(response.source_nodes)}\")\n",
    "    return response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522cfbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# 添加重排模型\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "reranker_model = r\"C:\\Users\\Administrator\\.cache\\modelscope\\hub\\models\\BAAI\\bge-reranker-large\"\n",
    "reranker = FlagEmbeddingReranker(\n",
    "    model=reranker_model, top_n=15\n",
    ")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "# 显式关闭全局 LLM 设置\n",
    "Settings.llm = None\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    llm=None,\n",
    "    response_mode=\"no_text\",\n",
    "    retriever=retriever, \n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac57131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving nodes for query: 技术开发项目中，可根据条件裁剪的角色有？\n",
      "20 nodes retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\Miniconda3\\envs\\raft\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " NodeWithScore(node=TextNode(id_='a3937eaa-b782-4512-863e-43c343b7ce31', embedding=None, metadata={'source': '2-对联苯-8-羟基喹啉锌...及其应用于新型白光OLED_赵婷_llm_correct.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='14 Ding, B.-D.; Zhang, J.-M.; Zhu, W.-Q.; Zheng, X.-Y.; Wu, Y.-Z.; Jiang, X.-Y.; Zhang, Z.-L.; Xu, S.-H.\\n\\nChin.\\n\\nJ.\\n\\nLumin.\\n\\n2003, 24, 606 (in Chinese).\\n\\n(丁邦东, 张积梅, 朱文清, 郑新友, 吴有智, 蒋雪茵, 张志林, 许少鸿, 发光学报, 2003, 24, 606.)\\n15 Flora, W.\\n\\nH.; Hall, H.\\n\\nK.; Armstrong, N.\\n\\nR.\\n\\nJ.\\n\\nPhys.\\n\\nChem.\\n\\nB 2003, 107, 1142.\\n\\n16 Kim, D.-E.; Kim, W.-S.; Kim, B.-S.; Lee, B.-J.; Kwon, Y.-S.\\n\\nColloids Surf.\\n\\nA: Physicochem.\\n\\nEng.\\n\\nAspects 2007, doi: 10.1016/j.colsurfa.2007.05.042.\\n\\n17 Shi, Y.-M.; Deng, Z.-B.; Xu, D.-H.; Xiao, J.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=-7.612576484680176))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query(\"技术开发项目中，可根据条件裁剪的角色有？\")\n",
    "len(response.source_nodes), response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_retriever = get_vector_retriever(\n",
    "    docstore_path=\"llama_index/docstore.json\",\n",
    "    embedding_path=\"\",\n",
    "    chroma_db=\"llama_index/chroma_db01\",\n",
    "    chroma_name=\"sc_collection\",\n",
    "    storage_dir=\"llama_index/vector_index01\",\n",
    ")\n",
    "retrieved_nodes = custom_retriever.retrieve(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import Settings, StorageContext, VectorStoreIndex\n",
    "\n",
    "\n",
    "def get_retriever(\n",
    "    docstore_path,\n",
    "    embedding_path,\n",
    "    chroma_db=\"llama_index/chroma_db01\",\n",
    "    chroma_name=\"sc_collection\",\n",
    "    storage_dir=\"llama_index/vector_index01\",\n",
    "    similarity_top_k=10\n",
    "):\n",
    "    docstore = SimpleDocumentStore.from_persist_path(docstore_path)\n",
    "    db = chromadb.PersistentClient(path=chroma_db)\n",
    "    chroma_collection = db.get_or_create_collection(name=chroma_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=storage_dir,\n",
    "        vector_store=vector_store, \n",
    "        docstore=docstore\n",
    "    )\n",
    "\n",
    "    doubao_embedding = get_doubao_embedding()\n",
    "    vector_index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=doubao_embedding\n",
    "        show_progress=True,\n",
    "    )\n",
    "    vector_retriever = vector_index.as_retriever(\n",
    "        similarity_top_k=similarity_top_k, \n",
    "        verbose=True\n",
    "    )\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        docstore=docstore,\n",
    "        similarity_top_k=similarity_top_k,\n",
    "    )\n",
    "    \n",
    "    custom_retriever = CustomRetriever(\n",
    "        vector_retriever, \n",
    "        bm25_retriever, \n",
    "    )\n",
    "    return custom_retriever\n",
    "\n",
    "# from llama_index.retrievers.bm25 import BM25Retriever\n",
    "# bm25_retriever = BM25Retriever.from_defaults(\n",
    "#     docstore=docstore,\n",
    "#     similarity_top_k=10,\n",
    "# )\n",
    "# bm25_retriever.persist(\"llama_index/bm25_retriever.json\")\n",
    "# loaded_bm25_retriever = BM25Retriever.from_persist_dir(\"llama_index/bm25_retriever.json\")\n",
    "\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import QueryBundle\n",
    "from typing import List\n",
    "\n",
    "# 4. 创建自定义的检索器\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"custom retriever that performs both vector and keyword table retrieval\"\"\"\n",
    "    def __init__(self,\n",
    "                 vector_retriever: VectorIndexRetriever,\n",
    "                 bm25_retriever: BM25Retriever,\n",
    "                 mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        if mode not in [\"AND\", \"OR\"]:\n",
    "            raise ValueError(\"mode must be either AND or OR\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"retrieve nodes given query\"\"\"\n",
    "        print(f\"Retrieving nodes for query: {query_bundle.query_str}\")\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        bm25_nodes = self._bm25_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        vector_ids = {node.node.node_id for node in vector_nodes}\n",
    "        bm25_ids = {node.node.node_id for node in bm25_nodes}\n",
    "        \n",
    "        combined_dict = {node.node.node_id: node for node in vector_nodes}\n",
    "        combined_dict.update({node.node.node_id: node for node in bm25_nodes})\n",
    "        \n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(bm25_ids)\n",
    "        if self._mode == \"OR\":\n",
    "            retrieve_ids = vector_ids.union(bm25_ids)\n",
    "        \n",
    "        retrieve_nodes = [combined_dict[node_id] for node_id in retrieve_ids]\n",
    "        print(f\"{len(retrieve_nodes)} nodes retrieved\")\n",
    "        return retrieve_nodes\n",
    "\n",
    "custom_retriever = CustomRetriever(\n",
    "    vector_retriever, \n",
    "    bm25_retriever, \n",
    ")\n",
    "retrieved_nodes = custom_retriever.retrieve(\"What is the capital of France?\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
