#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®è´¨é‡è¯„ä¼°å·¥å…·
åŸºäºåŸæœ‰çš„cotæ•°æ®è´¨é‡è¯„ä¼°ï¼Œå¢åŠ å¯¹æ¯”åˆ†æå’Œç»Ÿè®¡åŠŸèƒ½
ç›®æ ‡ï¼šè¯„ä¼°ç”Ÿæˆçš„é«˜è´¨é‡é—®ç­”å¯¹
"""

import os
import json
import pandas as pd
from openai import OpenAI
import datetime
import logging
from tqdm import tqdm
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å®¢æˆ·ç«¯ - ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
client = OpenAI(
    base_url=os.getenv("COMPLETION_OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("COMPLETION_OPENAI_API_KEY", "EMPTY")
)

def get_quality_evaluation_prompt():
    """è·å–è´¨é‡è¯„ä¼°prompt"""
    return """
ä½œä¸ºåŠå¯¼ä½“æ˜¾ç¤ºé¢†åŸŸçš„ä¸“ä¸šè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ‡å‡†è¯„ä¼°é—®ç­”å¯¹çš„è´¨é‡ã€‚è¯„ä¼°åˆ†ä¸ºæ ¸å¿ƒç»´åº¦ï¼Œæ¯ä¸ªç»´åº¦åŒ…å«å…·ä½“è¯„ä¼°ç‚¹å’Œç¤ºä¾‹å‚è€ƒã€‚

### è¯„ä¼°ç»´åº¦
1. æ€ç»´é“¾é€»è¾‘è´¨é‡ï¼ˆæƒé‡35%ï¼‰
   - æ­¥éª¤å®Œæ•´æ€§ï¼šæ¨ç†æ­¥éª¤æ˜¯å¦è¦†ç›–é—®é¢˜æ‰€æœ‰å…³é”®ç‚¹ï¼Ÿæ˜¯å¦é—æ¼å¿…è¦ç¯èŠ‚ï¼Ÿ
   - å› æœè¿è´¯æ€§ï¼šå‰åæ­¥éª¤æ˜¯å¦å­˜åœ¨æ¸…æ™°å› æœå…³ç³»ï¼Ÿæœ‰æ— é€»è¾‘æ–­è£‚ï¼Ÿ
   - æŠ€æœ¯å‚æ•°åˆç†æ€§ï¼šå·¥è‰ºå‚æ•°æ˜¯å¦ç¬¦åˆç‰©ç†è§„å¾‹ï¼Ÿï¼ˆä¾‹ï¼šLTPSé€€ç«æ¸©åº¦ä¸èƒ½è¶…è¿‡ç»ç’ƒè½¬åŒ–ç‚¹ï¼‰
   - é”™è¯¯å›æº¯æœºåˆ¶ï¼šæ˜¯å¦è€ƒè™‘å¯èƒ½æ•…éšœç‚¹ï¼Ÿï¼ˆä¾‹ï¼šåˆ†æMuraç¼ºé™·åº”åŒ…å«è®¾å¤‡ç²¾åº¦å› ç´ ï¼‰

2. æŠ€æœ¯å‡†ç¡®åº¦ï¼ˆæƒé‡30%ï¼‰
   - ææ–™ç‰¹æ€§ï¼šææ–™æè¿°æ˜¯å¦ç¬¦åˆç‰©æ€§ï¼Ÿï¼ˆä¾‹ï¼šIGZOè¿ç§»ç‡èŒƒå›´æ˜¯å¦æ­£ç¡®ï¼‰
   - åˆ¶ç¨‹å‚æ•°ï¼šå·¥è‰ºå‚æ•°æ˜¯å¦åŒ¹é…è¡Œä¸šæ ‡å‡†ï¼Ÿï¼ˆä¾‹ï¼šå…‰åˆ»ç²¾åº¦æ˜¯å¦æ»¡è¶³å½“å‰äº§çº¿èƒ½åŠ›ï¼‰
   - æ ‡å‡†å¼•ç”¨ï¼šæ˜¯å¦å‡†ç¡®å¼•ç”¨SEMI/SIDç­‰å›½é™…æ ‡å‡†ï¼Ÿ
   - ä¸“åˆ©æŠ€æœ¯ï¼šæŠ€æœ¯æ–¹æ¡ˆæ˜¯å¦è§„é¿è¿‘æœŸä¸“åˆ©ï¼Ÿ

3. é¢†åŸŸæ·±åº¦ï¼ˆæƒé‡20%ï¼‰
   - ç¼ºé™·æœºç†ï¼šæ˜¯å¦åˆ†ææ ¹æœ¬åŸå› ï¼Ÿï¼ˆä¾‹ï¼šäº®æš—ç‚¹åº”å…³è”ç”µè‡´è¿ç§»æœºåˆ¶ï¼‰
   - æŠ€æœ¯è¶‹åŠ¿ï¼šæ˜¯å¦è¦†ç›–æœ€æ–°å‘å±•ï¼Ÿï¼ˆä¾‹ï¼šéœ€æåŠMicro LEDå·¨é‡è½¬ç§»æŠ€æœ¯ï¼‰
   - å·¥è‰ºç“¶é¢ˆï¼šæ˜¯å¦è¯†åˆ«å…³é”®é™åˆ¶ï¼Ÿï¼ˆä¾‹ï¼šæŒ‡å‡ºQD-OLEDçš„å–·å¢¨æ‰“å°ç²¾åº¦ç“¶é¢ˆï¼‰

4. åº”ç”¨ä»·å€¼ï¼ˆæƒé‡15%ï¼‰
   - å·¥ç¨‹å¯è¡Œæ€§ï¼šæ–¹æ¡ˆæ˜¯å¦å…·å¤‡é‡äº§å®æ–½æ¡ä»¶ï¼Ÿ
   - æˆæœ¬ä¼˜åŒ–ï¼šæ˜¯å¦é‡åŒ–æˆæœ¬æ•ˆç›Šï¼Ÿ
   - è‰¯ç‡æå‡è·¯å¾„ï¼šæ˜¯å¦æä¾›å¯éªŒè¯çš„æ”¹å–„æ–¹æ¡ˆï¼Ÿ

### è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆJSONï¼‰
{{
    "quality_rating": {{
        "overall": "high/medium/low",
        "detailed_scores": {{
            "reasoning_chain": {{"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜1", "å…·ä½“é—®é¢˜2"]}},
            "technical_accuracy": {{"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜1"]}},
            "domain_depth": {{"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜1"]}},
            "application_value": {{"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜1"]}}
        }}
    }},
    "improvement_suggestions": ["å»ºè®®1", "å»ºè®®2"],
    "confidence_level": "high/medium/low"
}}

é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š{answer}
"""

def evaluate_qa_pair(question, answer, model_name="gpt-4"):
    """è¯„ä¼°å•ä¸ªé—®ç­”å¯¹çš„è´¨é‡"""
    try:
        prompt = get_quality_evaluation_prompt().format(question=question, answer=answer)
        
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠå¯¼ä½“æ˜¾ç¤ºæŠ€æœ¯é¢†åŸŸè´¨é‡è¯„ä¼°ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=2048
        )
        
        result = response.choices[0].message.content.strip()
        
        # å°è¯•è§£æJSONç»“æœ
        try:
            evaluation = json.loads(result)
            return evaluation
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•è§£æè¯„ä¼°ç»“æœJSON: {result[:100]}...")
            return None
            
    except Exception as e:
        logger.error(f"è¯„ä¼°é—®ç­”å¯¹æ—¶å‡ºé”™: {e}")
        return None

def load_syndatas(file_path):
    """åŠ è½½åˆæˆæ•°æ®æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}, åŒ…å« {len(data)} ä¸ªé—®ç­”å¯¹")
        return data
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return []

def evaluate_dataset(data_file, output_file, sample_size=None, model_name="gpt-4"):
    """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†çš„è´¨é‡"""
    logger.info(f"å¼€å§‹è¯„ä¼°æ•°æ®é›†: {data_file}")
    
    # åŠ è½½æ•°æ®
    data = load_syndatas(data_file)
    if not data:
        logger.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè¯„ä¼°")
        return
    
    # å¦‚æœæŒ‡å®šäº†æ ·æœ¬å¤§å°ï¼Œåˆ™éšæœºé‡‡æ ·
    if sample_size and sample_size < len(data):
        import random
        data = random.sample(data, sample_size)
        logger.info(f"éšæœºé‡‡æ · {sample_size} ä¸ªé—®ç­”å¯¹è¿›è¡Œè¯„ä¼°")
    
    evaluations = []
    
    for i, item in enumerate(tqdm(data, desc="è¯„ä¼°é—®ç­”å¯¹")):
        try:
            question = item.get('question', '')
            answer = item.get('reasoning_answer', '') or item.get('answer', '')
            
            if not question or not answer:
                logger.warning(f"ç¬¬ {i} ä¸ªé¡¹ç›®ç¼ºå°‘é—®é¢˜æˆ–ç­”æ¡ˆï¼Œè·³è¿‡")
                continue
            
            # è¯„ä¼°é—®ç­”å¯¹
            evaluation = evaluate_qa_pair(question, answer, model_name)
            
            if evaluation:
                evaluation['item_index'] = i
                evaluation['question'] = question[:100] + "..." if len(question) > 100 else question
                evaluations.append(evaluation)
            else:
                logger.warning(f"ç¬¬ {i} ä¸ªé—®ç­”å¯¹è¯„ä¼°å¤±è´¥")
                
        except Exception as e:
            logger.error(f"è¯„ä¼°ç¬¬ {i} ä¸ªé—®ç­”å¯¹æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluations, f, ensure_ascii=False, indent=2)
    
    logger.info(f"è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
    logger.info(f"æˆåŠŸè¯„ä¼° {len(evaluations)}/{len(data)} ä¸ªé—®ç­”å¯¹")
    
    return evaluations

def analyze_evaluation_results(evaluation_file):
    """åˆ†æè¯„ä¼°ç»“æœå¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    try:
        with open(evaluation_file, 'r', encoding='utf-8') as f:
            evaluations = json.load(f)
    except Exception as e:
        logger.error(f"åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
        return
    
    if not evaluations:
        logger.error("è¯„ä¼°ç»“æœä¸ºç©º")
        return
    
    # ç»Ÿè®¡æ•´ä½“è´¨é‡åˆ†å¸ƒ
    overall_scores = defaultdict(int)
    dimension_scores = defaultdict(lambda: defaultdict(int))
    confidence_levels = defaultdict(int)
    
    for eval_result in evaluations:
        # æ•´ä½“è¯„åˆ†ç»Ÿè®¡
        overall = eval_result.get('quality_rating', {}).get('overall', 'unknown')
        overall_scores[overall] += 1
        
        # å„ç»´åº¦è¯„åˆ†ç»Ÿè®¡
        detailed_scores = eval_result.get('quality_rating', {}).get('detailed_scores', {})
        for dimension, score_info in detailed_scores.items():
            score = score_info.get('score', 'unknown')
            dimension_scores[dimension][score] += 1
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidence = eval_result.get('confidence_level', 'unknown')
        confidence_levels[confidence] += 1
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    total_count = len(evaluations)
    
    print("\n" + "="*50)
    print("ğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°ç»Ÿè®¡æŠ¥å‘Š")
    print("="*50)
    
    print(f"\nğŸ“ˆ æ•´ä½“è´¨é‡åˆ†å¸ƒ (æ€»è®¡: {total_count} ä¸ªé—®ç­”å¯¹):")
    for score, count in sorted(overall_scores.items()):
        percentage = (count / total_count) * 100
        print(f"   {score:8}: {count:4} ({percentage:5.1f}%)")
    
    print(f"\nğŸ” å„ç»´åº¦è´¨é‡åˆ†å¸ƒ:")
    for dimension, scores in dimension_scores.items():
        print(f"\n   {dimension}:")
        for score, count in sorted(scores.items()):
            percentage = (count / total_count) * 100
            print(f"     {score:8}: {count:4} ({percentage:5.1f}%)")
    
    print(f"\nğŸ¯ è¯„ä¼°ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    for confidence, count in sorted(confidence_levels.items()):
        percentage = (count / total_count) * 100
        print(f"   {confidence:8}: {count:4} ({percentage:5.1f}%)")
    
    # è®¡ç®—é«˜è´¨é‡é—®ç­”å¯¹æ¯”ä¾‹
    high_quality_count = overall_scores.get('high', 0)
    high_quality_rate = (high_quality_count / total_count) * 100
    
    print(f"\nâœ¨ å…³é”®æŒ‡æ ‡:")
    print(f"   é«˜è´¨é‡é—®ç­”å¯¹æ•°é‡: {high_quality_count}")
    print(f"   é«˜è´¨é‡é—®ç­”å¯¹æ¯”ä¾‹: {high_quality_rate:.1f}%")
    
    # æå–å¸¸è§é—®é¢˜
    common_issues = defaultdict(int)
    for eval_result in evaluations:
        detailed_scores = eval_result.get('quality_rating', {}).get('detailed_scores', {})
        for dimension, score_info in detailed_scores.items():
            issues = score_info.get('issues', [])
            for issue in issues:
                common_issues[issue] += 1
    
    if common_issues:
        print(f"\nâš ï¸  å¸¸è§é—®é¢˜ (TOP 10):")
        sorted_issues = sorted(common_issues.items(), key=lambda x: x[1], reverse=True)
        for i, (issue, count) in enumerate(sorted_issues[:10], 1):
            print(f"   {i:2}. {issue} ({count} æ¬¡)")
    
    print("\n" + "="*50)
    
    return {
        "total_count": total_count,
        "overall_scores": dict(overall_scores),
        "dimension_scores": dict(dimension_scores),
        "confidence_levels": dict(confidence_levels),
        "high_quality_rate": high_quality_rate,
        "common_issues": dict(sorted_issues[:10])
    }

def compare_datasets(datasets_info):
    """å¯¹æ¯”å¤šä¸ªæ•°æ®é›†çš„è´¨é‡"""
    print("\n" + "="*60)
    print("ğŸ”„ æ•°æ®é›†è´¨é‡å¯¹æ¯”åˆ†æ")
    print("="*60)
    
    comparison_results = {}
    
    for name, file_path in datasets_info.items():
        print(f"\nğŸ“Š åˆ†ææ•°æ®é›†: {name}")
        eval_file = file_path.replace('.json', '_evaluation.json')
        
        if not os.path.exists(eval_file):
            print(f"   âš ï¸  è¯„ä¼°æ–‡ä»¶ä¸å­˜åœ¨: {eval_file}")
            continue
        
        stats = analyze_evaluation_results(eval_file)
        if stats:
            comparison_results[name] = stats
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    if len(comparison_results) > 1:
        print(f"\nğŸ“ˆ æ•°æ®é›†è´¨é‡å¯¹æ¯”:")
        print(f"{'æ•°æ®é›†':<20} {'æ€»æ•°':<8} {'é«˜è´¨é‡ç‡':<10} {'ç½®ä¿¡åº¦-é«˜':<10}")
        print("-" * 60)
        
        for name, stats in comparison_results.items():
            high_quality_rate = stats['high_quality_rate']
            high_confidence_count = stats['confidence_levels'].get('high', 0)
            high_confidence_rate = (high_confidence_count / stats['total_count']) * 100
            
            print(f"{name:<20} {stats['total_count']:<8} {high_quality_rate:<9.1f}% {high_confidence_rate:<9.1f}%")
    
    return comparison_results

def get_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæ•°æ®è´¨é‡è¯„ä¼°å·¥å…·")
    
    parser.add_argument("--input", type=str, help="å•ä¸ªæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--compare", nargs='+', help="å¤šä¸ªæ•°æ®æ–‡ä»¶è·¯å¾„è¿›è¡Œå¯¹æ¯”")
    parser.add_argument("--output_dir", type=str, default="evaluation_results", help="è¯„ä¼°ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--sample_size", type=int, help="è¯„ä¼°æ ·æœ¬å¤§å°ï¼ˆç”¨äºå¤§æ•°æ®é›†é‡‡æ ·ï¼‰")
    parser.add_argument("--model", type=str, default="gpt-4", help="è¯„ä¼°ä½¿ç”¨çš„æ¨¡å‹")
    parser.add_argument("--skip_evaluation", action="store_true", help="è·³è¿‡è¯„ä¼°ï¼Œä»…åˆ†æå·²æœ‰ç»“æœ")
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = get_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    if args.input:
        # å•æ–‡ä»¶è¯„ä¼°
        input_path = Path(args.input)
        output_file = os.path.join(args.output_dir, f"{input_path.stem}_evaluation.json")
        
        if not args.skip_evaluation:
            evaluations = evaluate_dataset(args.input, output_file, args.sample_size, args.model)
        
        # åˆ†æç»“æœ
        analyze_evaluation_results(output_file)
        
    elif args.compare:
        # å¤šæ–‡ä»¶å¯¹æ¯”è¯„ä¼°
        datasets_info = {}
        
        for file_path in args.compare:
            file_path = Path(file_path)
            name = file_path.stem
            datasets_info[name] = str(file_path)
            
            if not args.skip_evaluation:
                output_file = os.path.join(args.output_dir, f"{name}_evaluation.json")
                evaluate_dataset(str(file_path), output_file, args.sample_size, args.model)
        
        # å¯¹æ¯”åˆ†æ
        compare_datasets(datasets_info)
        
    else:
        # é»˜è®¤è¯„ä¼°å¢å¼ºç‰ˆç®¡é“çš„è¾“å‡º
        default_files = {
            "enhanced": "outputs_enhanced/syndatas/syndatas_enhanced.json",
            "fast": "outputs_enhanced/syndatas/syndatas_fast.json",
            "compat": "outputs_enhanced/syndatas/syndatas_compat.json"
        }
        
        existing_files = {name: path for name, path in default_files.items() if os.path.exists(path)}
        
        if not existing_files:
            logger.error("æœªæ‰¾åˆ°é»˜è®¤çš„æ•°æ®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ --input æˆ– --compare å‚æ•°")
            return
        
        logger.info(f"æ‰¾åˆ° {len(existing_files)} ä¸ªé»˜è®¤æ•°æ®æ–‡ä»¶è¿›è¡Œè¯„ä¼°")
        
        if not args.skip_evaluation:
            for name, file_path in existing_files.items():
                output_file = os.path.join(args.output_dir, f"{name}_evaluation.json")
                evaluate_dataset(file_path, output_file, args.sample_size, args.model)
        
        # å¯¹æ¯”åˆ†æ
        compare_datasets(existing_files)

if __name__ == "__main__":
    main()