#!/bin/bash

# 本地模型使用示例脚本
# 展示如何配置和运行不同的本地大模型

echo "🚀 本地模型数据合成管道示例"
echo "=================================================="

# 基础配置
DATA_DIR="data"
OUTPUT_BASE="outputs_local"
BATCH_SIZE=16
MAX_WORKERS=2

# 示例1: 使用预定义的模型（如果路径已在vllm_client.py中配置）
echo "📌 示例1: 使用预定义的QwQ-32B模型"
echo "bash命令:"
echo "python -m utils.syndata_pipeline_v5 \\"
echo "  --data_dir \"${DATA_DIR}\" \\"
echo "  --filtered_data_dir \"data_filtered_qwq\" \\"
echo "  --chunks_path \"${OUTPUT_BASE}/chunks/article_chunks_qwq.json\" \\"
echo "  --chunk4_path \"${OUTPUT_BASE}/chunk4/article_chunk4_qwq.json\" \\"
echo "  --topics_path \"${OUTPUT_BASE}/topics/article_topics_qwq.json\" \\"
echo "  --questions_path \"${OUTPUT_BASE}/questions/article_questions_qwq.json\" \\"
echo "  --validated_questions_path \"${OUTPUT_BASE}/validated_questions/article_questions_qwq_validated.json\" \\"
echo "  --answers_path \"${OUTPUT_BASE}/answers/article_answers_qwq.json\" \\"
echo "  --syndatas_path \"${OUTPUT_BASE}/syndatas/syndatas_qwq.json\" \\"
echo "  --start_idx 0 \\"
echo "  --end_idx 10 \\"
echo "  --use_vllm \\"
echo "  --model_name qwq_32 \\"
echo "  --gpu_memory_utilization 0.9 \\"
echo "  --tensor_parallel_size 4"
echo ""

# 示例2: 使用自定义路径的Qwen2模型
echo "📌 示例2: 使用自定义路径的Qwen2-7B模型"
echo "bash命令:"
echo "python -m utils.syndata_pipeline_v5 \\"
echo "  --data_dir \"${DATA_DIR}\" \\"
echo "  --filtered_data_dir \"data_filtered_qwen2\" \\"
echo "  --chunks_path \"${OUTPUT_BASE}/chunks/article_chunks_qwen2.json\" \\"
echo "  --chunk4_path \"${OUTPUT_BASE}/chunk4/article_chunk4_qwen2.json\" \\"
echo "  --topics_path \"${OUTPUT_BASE}/topics/article_topics_qwen2.json\" \\"
echo "  --questions_path \"${OUTPUT_BASE}/questions/article_questions_qwen2.json\" \\"
echo "  --validated_questions_path \"${OUTPUT_BASE}/validated_questions/article_questions_qwen2_validated.json\" \\"
echo "  --answers_path \"${OUTPUT_BASE}/answers/article_answers_qwen2.json\" \\"
echo "  --syndatas_path \"${OUTPUT_BASE}/syndatas/syndatas_qwen2.json\" \\"
echo "  --start_idx 0 \\"
echo "  --end_idx 10 \\"
echo "  --use_vllm \\"
echo "  --model_name qwen2-7b \\"
echo "  --model_path \"/path/to/your/Qwen2-7B-Instruct\" \\"
echo "  --gpu_memory_utilization 0.9 \\"
echo "  --tensor_parallel_size 1 \\"
echo "  --max_model_len 32768"
echo ""

# 示例3: 使用LLaMA模型
echo "📌 示例3: 使用LLaMA-3-8B模型"
echo "bash命令:"
echo "python -m utils.syndata_pipeline_v5 \\"
echo "  --data_dir \"${DATA_DIR}\" \\"
echo "  --filtered_data_dir \"data_filtered_llama\" \\"
echo "  --chunks_path \"${OUTPUT_BASE}/chunks/article_chunks_llama.json\" \\"
echo "  --chunk4_path \"${OUTPUT_BASE}/chunk4/article_chunk4_llama.json\" \\"
echo "  --topics_path \"${OUTPUT_BASE}/topics/article_topics_llama.json\" \\"
echo "  --questions_path \"${OUTPUT_BASE}/questions/article_questions_llama.json\" \\"
echo "  --validated_questions_path \"${OUTPUT_BASE}/validated_questions/article_questions_llama_validated.json\" \\"
echo "  --answers_path \"${OUTPUT_BASE}/answers/article_answers_llama.json\" \\"
echo "  --syndatas_path \"${OUTPUT_BASE}/syndatas/syndatas_llama.json\" \\"
echo "  --start_idx 0 \\"
echo "  --end_idx 10 \\"
echo "  --use_vllm \\"
echo "  --model_name llama3-8b \\"
echo "  --model_path \"/path/to/your/Meta-Llama-3-8B-Instruct\" \\"
echo "  --gpu_memory_utilization 0.9 \\"
echo "  --tensor_parallel_size 1"
echo ""

# 示例4: 使用完全自定义的模型
echo "📌 示例4: 使用完全自定义的模型"
echo "bash命令:"
echo "python -m utils.syndata_pipeline_v5 \\"
echo "  --data_dir \"${DATA_DIR}\" \\"
echo "  --filtered_data_dir \"data_filtered_custom\" \\"
echo "  --chunks_path \"${OUTPUT_BASE}/chunks/article_chunks_custom.json\" \\"
echo "  --chunk4_path \"${OUTPUT_BASE}/chunk4/article_chunk4_custom.json\" \\"
echo "  --topics_path \"${OUTPUT_BASE}/topics/article_topics_custom.json\" \\"
echo "  --questions_path \"${OUTPUT_BASE}/questions/article_questions_custom.json\" \\"
echo "  --validated_questions_path \"${OUTPUT_BASE}/validated_questions/article_questions_custom_validated.json\" \\"
echo "  --answers_path \"${OUTPUT_BASE}/answers/article_answers_custom.json\" \\"
echo "  --syndatas_path \"${OUTPUT_BASE}/syndatas/syndatas_custom.json\" \\"
echo "  --start_idx 0 \\"
echo "  --end_idx 10 \\"
echo "  --use_vllm \\"
echo "  --model_name custom \\"
echo "  --model_path \"/path/to/your/custom-model\" \\"
echo "  --gpu_memory_utilization 0.9 \\"
echo "  --tensor_parallel_size 2 \\"
echo "  --max_model_len 65536"
echo ""

echo "💡 提示："
echo "1. 修改 --model_path 为你的实际模型路径"
echo "2. 根据GPU内存调整 --batch_size 和 --gpu_memory_utilization"
echo "3. 根据GPU数量调整 --tensor_parallel_size"
echo "4. 根据模型能力调整 --max_model_len"
echo ""
echo "🔧 常见参数说明："
echo "  --model_name: 模型名称（qwq_32, qwen2-7b, llama3-8b, custom等）"
echo "  --model_path: 模型文件夹路径（包含config.json等文件）"
echo "  --tensor_parallel_size: 使用的GPU数量"
echo "  --gpu_memory_utilization: GPU内存使用率（0-1之间）"
echo "  --max_model_len: 最大上下文长度"
echo "  --batch_size: 批处理大小"
echo "  --skip_document_filter: 跳过文档质量筛选"
echo "  --skip_question_validation: 跳过问题验证"
echo ""
echo "=================================================="